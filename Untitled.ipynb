{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e66e03c3-248f-4a93-b49e-df18a01f2d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy data saved to data/scraped_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Let's scrape reviews for a book from 'books.toscrape.com'\n",
    "# NOTE: Real e-commerce sites are much harder to scrape. This is for learning.\n",
    "base_url = 'http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/reviews.html'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "response = requests.get(base_url, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all review blocks\n",
    "reviews = soup.find_all('article', class_='product_pod') # This class is for products, but we'll use it as an example. \n",
    "# On a real review page, you'd inspect the HTML to find the correct tags and classes for reviews.\n",
    "\n",
    "# For this example, let's create some dummy data as if we scraped it\n",
    "review_data = [\n",
    "    {'rating': 5, 'text': 'This is the best book I have ever read! Absolutely captivating.'},\n",
    "    {'rating': 4, 'text': 'A really great read, I enjoyed it a lot and would recommend.'},\n",
    "    {'rating': 3, 'text': 'It was an okay book, not bad but not memorable either.'},\n",
    "    {'rating': 2, 'text': 'I was disappointed. The plot was weak and the characters were flat.'},\n",
    "    {'rating': 1, 'text': 'Terrible. I could not finish it. A complete waste of time and money.'}\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(review_data)\n",
    "df.to_csv('data/scraped_reviews.csv', index=False)\n",
    "\n",
    "print(\"Dummy data saved to data/scraped_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d39851-4230-4412-abdc-bd93c1c185ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                               text sentiment  \\\n",
      "0       5  This is the best book I have ever read! Absolu...  Positive   \n",
      "1       4  A really great read, I enjoyed it a lot and wo...  Positive   \n",
      "2       3  It was an okay book, not bad but not memorable...   Neutral   \n",
      "3       2  I was disappointed. The plot was weak and the ...  Negative   \n",
      "4       1  Terrible. I could not finish it. A complete wa...  Negative   \n",
      "\n",
      "                                      cleaned_text  \n",
      "0       best book ever read absolutely captivating  \n",
      "1    really great read enjoyed lot would recommend  \n",
      "2                   okay book bad memorable either  \n",
      "3            disappointed plot weak character flat  \n",
      "4  terrible could finish complete waste time money  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('data/scraped_reviews.csv')\n",
    "\n",
    "# 1. Create sentiment labels from ratings\n",
    "def get_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return 'Positive'\n",
    "    elif rating == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "df['sentiment'] = df['rating'].apply(get_sentiment)\n",
    "\n",
    "# 2. Clean the text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words] # Lemmatize and remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e25c9e3f-888e-4362-afd6-ba26afd6e9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                               text sentiment  \\\n",
      "0       5  This is the best book I have ever read! Absolu...  Positive   \n",
      "1       4  A really great read, I enjoyed it a lot and wo...  Positive   \n",
      "2       3  It was an okay book, not bad but not memorable...   Neutral   \n",
      "3       2  I was disappointed. The plot was weak and the ...  Negative   \n",
      "4       1  Terrible. I could not finish it. A complete wa...  Negative   \n",
      "\n",
      "                                      cleaned_text  \n",
      "0       best book ever read absolutely captivating  \n",
      "1    really great read enjoyed lot would recommend  \n",
      "2                   okay book bad memorable either  \n",
      "3            disappointed plot weak character flat  \n",
      "4  terrible could finish complete waste time money  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('data/scraped_reviews.csv')\n",
    "\n",
    "# 1. Create sentiment labels from ratings\n",
    "def get_sentiment(rating):\n",
    "    if rating >= 4:\n",
    "        return 'Positive'\n",
    "    elif rating == 3:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "df['sentiment'] = df['rating'].apply(get_sentiment)\n",
    "\n",
    "# 2. Clean the text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation and numbers\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words] # Lemmatize and remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6eb9e610-f65d-445d-88f1-b25f3fe87f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vcs17\\sentiment_analysis_project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\vcs17\\sentiment_analysis_project\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vcs17\\.cache\\huggingface\\hub\\models--distilbert-base-uncased-finetuned-sst-2-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: 'best book ever read absolutely captivating'\n",
      "Predicted Sentiment: [{'label': 'POSITIVE', 'score': 0.9998810291290283}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained sentiment analysis model\n",
    "# This model is great for general-purpose sentiment analysis.\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "# Test it on a sample review\n",
    "sample_review = df['cleaned_text'].iloc[0]\n",
    "result = sentiment_pipeline(sample_review)\n",
    "\n",
    "print(f\"Review: '{sample_review}'\")\n",
    "print(f\"Predicted Sentiment: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6b462-6e22-491c-8e27-b0edda25503c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
